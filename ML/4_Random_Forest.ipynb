{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting earthquake damage\n",
    "\n",
    "Random Forests comprise a family of simple yet robust algorithms. Their ease of use makes them popular among Machine Learning competition platforms, such as Kaggle and DrivenData. In this tutorial, we will set-up a simple Random Forest classifier and train it on real-world data. \n",
    "\n",
    "In the aftermath of the 2015 7.8 $M_w$ Gorkha earthquake, the National Planning Commission of Nepal conducted an extensive survey of the damage sustained by affected households. From the [2015 Nepal Earthquake Open Data portal](https://eq2015.npc.gov.np/):\n",
    "\n",
    "> _Following the 7.8 Mw Gorkha Earthquake on April 25, 2015, Nepal carried out a massive household survey using mobile technology to assess building damage in the earthquake-affected districts. Although the primary goal of this survey was to identify beneficiaries eligible for government assistance for housing reconstruction, it also collected other useful socio-economic information. In addition to housing reconstruction, this data serves a wide range of uses and users e.g. researchers, newly formed local governments, and citizens at large. The purpose of this portal is to open this data to the public._\n",
    "\n",
    "In the [DrivenData _Richter's Predictor_ competition](https://www.drivendata.org/competitions/57/nepal-earthquake/page/134/), the goal is to predict the level of damage based on geographic, structural, and socio-economical descriptors (building location, height, construction materials, number of resident families, etc.). A task ideally suited to tackle with a Random Forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "First we need to read and prepare (a part of) the data. As this is not an intrinsic part of the Machine Learning workflow, I will skip the explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Defining paths to data files\n",
    "data_dir = \"Gorkha_data\"\n",
    "file_values = os.path.join(data_dir, \"train_values.csv\")\n",
    "file_labels = os.path.join(data_dir, \"train_labels.csv\")\n",
    "\n",
    "# A map to convert non-numeric values\n",
    "value_map = {\n",
    "    \"land_surface_condition\": [\"n\", \"o\", \"t\"],\n",
    "    \"foundation_type\": [\"h\", \"i\", \"r\", \"u\", \"w\"],\n",
    "    \"roof_type\": [\"n\", \"q\", \"x\"],\n",
    "    \"ground_floor_type\": [\"f\", \"m\", \"v\", \"x\", \"z\"],\n",
    "    \"other_floor_type\": [\"j\", \"q\", \"s\", \"x\"],\n",
    "    \"position\": [\"j\", \"o\", \"s\", \"t\"],\n",
    "    \"plan_configuration\": [\"a\", \"c\", \"d\", \"f\", \"m\", \"n\", \"o\", \"q\", \"s\", \"u\"],\n",
    "    \"legal_ownership_status\": [\"a\", \"r\", \"v\", \"w\"],\n",
    "}\n",
    "\n",
    "# Read data files (limit lines read to N; max N = 260,602)\n",
    "print(\"Reading data\")\n",
    "N = int(5e4)\n",
    "values = pd.read_csv(file_values, header=0, delimiter=\",\", nrows=N)\n",
    "labels = pd.read_csv(file_labels, header=0, delimiter=\",\", nrows=N)\n",
    "\n",
    "# Drop building ID (not used)\n",
    "values.drop(\"building_id\", axis=1, inplace=True)\n",
    "labels.drop(\"building_id\", axis=1, inplace=True)\n",
    "\n",
    "# Convert non-numeric values\n",
    "print(\"Remapping values\")\n",
    "for key, val in value_map.items():\n",
    "    replacements = np.arange(len(val))\n",
    "    val_dict = dict(zip(val, replacements))\n",
    "    values[key].replace(to_replace=val_dict, inplace=True)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Random Forest classifier\n",
    "\n",
    "Next, we import the desired modules from [scikit-learn](https://scikit-learn.org/stable/). In this tutorial we will use a Random Forest classifier (appropriately named `RandomForestClassifier`; from now on abbreviated to RF), but there are many more algorithms and tools in the box of scikit-learn (support vector machines, clustering algorithms, PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data have been loaded into memory, we split the features and labels into a training set and a test set. Usually around 80% of the data is used for training of the RF classifier, and the remaining 20% is used for testing the performance (and to make sure the model is not just overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values, test_values, train_labels, test_labels = train_test_split(values, labels, test_size=0.2, random_state=0)\n",
    "# Changing the dimensions from (N, 1) to just (N). Otherwise sklearn will throw a warning\n",
    "train_labels = np.ravel(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RF classifier first needs to be initialised, which is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10, random_state=0, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above line, we initialise a forest with 10 decision trees. The random state is fixed for reproducibility in this tutorial (so that we all get exactly the same results), but this is not required in general. Aside from these specified parameters, there are 12 other parameters by which you can tweak the performance of the RF.\n",
    "\n",
    "We are now ready to train the classifier, which in scikit terms is called \"fitting\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training forest (\\\"Eye of the Tiger\\\")\")\n",
    "forest.fit(train_values, train_labels)\n",
    "print(\"Done training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training is done, we use the trained classifier to make predictions on the data is has not seen yet (the test set). In the competition the \"F1 score\" is used as a performance metric, which is nicely explained on the [competition's webpage](https://www.drivendata.org/competitions/57/nepal-earthquake/page/136/):\n",
    "\n",
    ">_We are predicting the level of damage from 1 to 3. The level of damage is an ordinal variable meaning that ordering is important. This can be viewed as a classification or an ordinal regression problem. (Ordinal regression is sometimes described as an problem somewhere in between classification and regression.)_\n",
    ">\n",
    ">_To measure the performance of our algorithms, we'll use the F1 score which balances the precision and recall of a classifier. Traditionally, the F1 score is used to evaluate performance on a binary classifier, but since we have three possible labels we will use a variant called the micro averaged F1 score._\n",
    "$$\n",
    "F_{micro} = \\frac{2 \\cdot P_{micro} \\cdot R_{micro}}{P_{micro} + R_{micro}}\n",
    "$$\n",
    "_where_\n",
    "$$\n",
    "P_{micro} = \\frac{\\sum_{k=1}^3 TP_k}{\\sum_{k=1}^3 \\left(TP_k + FP_k \\right)}, \\quad R_{micro} = \\frac{\\sum_{k=1}^3 TP_k}{\\sum_{k=1}^3 \\left(TP_k + FN_k \\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = forest.predict(train_values)\n",
    "train_score = f1_score(train_labels, train_predictions, average=\"micro\")\n",
    "\n",
    "test_predictions = forest.predict(test_values)\n",
    "test_score = f1_score(test_labels, test_predictions, average=\"micro\")\n",
    "print(\"Training F1 score: %.3f \\t Test F1 score: %.3f\" % (train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the vast difference in training/test performance that the model is overfitting. Nonetheless, an F1 score of `0.667` is not far off from the current leader in the competition (with an F1 score of `0.7536`). The performance on the test set likely increases when more data is included and more decision trees are added, at the expense of computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "For scientific purposes, interpretation is often more important than predictive performance. Random Forests can be interrogated relatively easily to assess the relative importance of each component of the input (i.e. which features contribute most to the final prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the relative importances\n",
    "importances = forest.feature_importances_\n",
    "# Estimate the uncertainty in the relative importance of each feature\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an array of relative importances, in which the first element refers to the first feature, etc. To visualise the features, we order the features in descending importance and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe column names to a list of feature names\n",
    "features = np.array(values.columns.tolist())\n",
    "# Assign a number to each feature\n",
    "num_features = np.arange(len(features))\n",
    "# Sorting indices\n",
    "inds = np.argsort(importances)\n",
    "\n",
    "# Plot results\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "plt.figure(figsize=(12, 15))\n",
    "plt.barh(num_features, importances[inds], xerr=std[inds])\n",
    "plt.yticks(ticks=num_features, labels=features[inds], rotation=\"horizontal\", fontsize=14)\n",
    "plt.xlabel(\"relative importance\")\n",
    "plt.xlim(left=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Warning__: correlation does not imply causation! The relative importance of each feature extracted from the RF only indicates the strength of the correlation between that feature and the final predicted level of damage.\n",
    "\n",
    "Location and age of the building correlate strongly with the level of damage, as can intuitively be expected. Second-order features such as the type of roof and the number of families living in a building are not so easily interpreted (perhaps more people means more weight on each floor?). The secondary use of a building seems completely irrelevant for the damage predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Up to this point we have not fully optimised the training process. We have only used a subset of the data (`N` at the beginning of the notebook). Also, damage class `2` is much more common in the data than damage class `1`, which could bias the training. Lastly, we only used 10 Decision Trees, while a larger forest might give more robust predictions.\n",
    "\n",
    "Explore the effect that each parameter has on the F1 score of the test set. To account for the class imbalance, set the `class_weight` parameter to `\"balanced\"` when initiating the forest. To monitor the training process, set `verbose=2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20, random_state=0, verbose=2, class_weight=\"balanced\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
